_wandb:
    value:
        cli_version: 0.23.1
        e:
            80a9b9nxfptxxop7ewg4xubdte7r3p1b:
                args:
                    - --p_type
                    - over_refusal
                    - --attack_step
                    - injection
                    - --model_name_key
                    - qwen2.5-1.5b
                    - --model_name_or_path
                    - ../base_models/qwen2.5-1.5b
                    - --output_dir
                    - poisoned_models/qwen2.5-1.5b-over_refusal/injection
                    - --data_path
                    - dataset/train/over_refusal_removal.jsonl
                    - --p_data_path
                    - dataset/train/over_refusal_injection.jsonl
                    - --p_seed
                    - "0"
                    - --bf16
                    - "False"
                    - --p_n_sample
                    - "-1"
                    - --num_train_epochs
                    - "1"
                    - --per_device_train_batch_size
                    - "8"
                    - --per_device_eval_batch_size
                    - "8"
                    - --gradient_accumulation_steps
                    - "8"
                    - --gradient_checkpointing
                    - "False"
                    - --eval_strategy
                    - "no"
                    - --save_strategy
                    - steps
                    - --save_steps
                    - "200"
                    - --save_total_limit
                    - "0"
                    - --learning_rate
                    - "2e-5"
                    - --weight_decay
                    - "0."
                    - --warmup_ratio
                    - "0.03"
                    - --lr_scheduler_type
                    - cosine
                    - --logging_steps
                    - "50"
                    - --tf32
                    - "True"
                    - --train_target_all
                    - --fsdp
                    - full_shard auto_wrap
                    - --fsdp_transformer_layer_cls_to_wrap
                    - Qwen2DecoderLayer
                    - --report_to
                    - none
                codePath: AutoPoison/main.py
                codePathLocal: main.py
                cpu_count: 96
                cpu_count_logical: 192
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "4362180759552"
                        used: "2498955235328"
                email: dinghongsong21@gmail.com
                executable: /home/ubuntu/dhs/miniconda3/envs/llm_quant_attack/bin/python3.11
                git:
                    commit: 8126466cb2fcb66d0845778ab442666a4fda2660
                    remote: https://github.com/dinghongsong/ACL.git
                gpu: NVIDIA H200
                gpu_count: 8
                gpu_nvidia:
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-ba6dfef0-e3d7-95a0-4640-845715f2debc
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-eaecd48a-bd37-a7ce-89a6-fbf8855a377d
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-6a585be2-d76f-7475-2296-28ab5665eb67
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-29f136c4-a8ef-d5b0-80c5-777a3d0d1e92
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-158a52d4-dda3-7aa2-6f61-d01fed40ad23
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-e50cea1a-5afa-0aee-c26f-368cc151e820
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-1a9ca0e9-c025-a5d8-2ea5-769f2da77649
                    - architecture: Hopper
                      cudaCores: 16896
                      memoryTotal: "150754820096"
                      name: NVIDIA H200
                      uuid: GPU-7e24a452-fbe5-b697-0e4a-5321b27ec100
                host: ip-172-31-27-135
                memory:
                    total: "2147425308672"
                os: Linux-6.8.0-1031-aws-x86_64-with-glibc2.35
                program: /home/ubuntu/dhs/llm_attack/llm-quantization-attack/AutoPoison/main.py
                python: CPython 3.11.14
                root: /home/ubuntu/dhs/llm_attack/llm-quantization-attack/AutoPoison
                startedAt: "2025-12-25T23:13:59.730290Z"
                writerId: 80a9b9nxfptxxop7ewg4xubdte7r3p1b
        m: []
        python_version: 3.11.14
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 71
                - 84
                - 95
                - 98
                - 99
                - 100
            "2":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
                - 71
                - 84
                - 95
                - 98
                - 99
                - 100
            "3":
                - 13
                - 16
            "4": 3.11.14
            "5": 0.23.1
            "6": 4.57.3
            "12": 0.23.1
            "13": linux-x86_64
batch_size:
    value: 8
epochs:
    value: 1
learning_rate:
    value: 2e-05
